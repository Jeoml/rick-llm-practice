{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhAJBkZKiyyH"
      },
      "source": [
        "This notebook contains a notebook version of the finetune process. We'll do exactly the same but using GCP instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "AEitO1fXjBiy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: unsloth in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (2024.12.12)\n",
            "Requirement already satisfied: xformers in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (0.0.27.post2)\n",
            "Requirement already satisfied: filelock in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: unsloth_zoo>=2024.12.7 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (2024.12.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (2.4.0)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.46.1)\n",
            "Requirement already satisfied: tyro in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.9.26)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (4.53.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (7.0.0)\n",
            "Requirement already satisfied: wheel>=0.42.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (1.8.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.19.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.16.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: hf_transfer in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from torch>=2.4.0->unsloth) (4.14.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: colorama in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.2)\n",
            "Requirement already satisfied: cut_cross_entropy in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth_zoo>=2024.12.7->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from unsloth_zoo>=2024.12.7->unsloth) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from sympy->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from tyro->unsloth) (14.0.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from tyro->unsloth) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joel1\\desktop\\agentic_learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets unsloth xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Iyxg8t8ABRz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2b8f2337854de5be0542d638b3ac9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY0pv4a3FvbG"
      },
      "source": [
        "First of all, we are going to load the dataset containing Rick & Morty transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g1IPNHYvGHBA"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_sharegpt\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheneuralmaze/rick-and-morty-transcripts-sharegpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m standardize_sharegpt(dataset)\n",
            "File \u001b[1;32mc:\\Users\\joel1\\Desktop\\Agentic_Learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages\\unsloth\\__init__.py:93\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Torch 2.4 has including_emulation\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m SUPPORTS_BFLOAT16 \u001b[38;5;241m=\u001b[39m (major_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     96\u001b[0m old_is_bf16_supported \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_bf16_supported\n",
            "File \u001b[1;32mc:\\Users\\joel1\\Desktop\\Agentic_Learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:451\u001b[0m, in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
            "File \u001b[1;32mc:\\Users\\joel1\\Desktop\\Agentic_Learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:465\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
            "File \u001b[1;32mc:\\Users\\joel1\\Desktop\\Agentic_Learning\\neural-hub\\rick-llm\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "dataset = load_dataset(\"theneuralmaze/rick-and-morty-transcripts-sharegpt\", split=\"train\")\n",
        "dataset = standardize_sharegpt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2E8Gj75mfnw"
      },
      "outputs": [],
      "source": [
        "print(\"Number of rows: \", len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPhJgxYomaY3"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei0rXX5tGcvb"
      },
      "source": [
        "Now, let's load both the model (Llama 3.1 8B) and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36CypqJwjo5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=False,\n",
        "    dtype=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTPQpy0oGzv1"
      },
      "source": [
        "Instead of a full finetuning, we are going to use LoRa finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oVSVa7nl1bf"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1twwKyscHx2S"
      },
      "source": [
        "The next line of code will generate a new column (`text`), that contains the data in the format needed for the finetune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meJjl4Qins12"
      },
      "outputs": [],
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "chat_template = \"\"\"<|im_start|>system\n",
        "{SYSTEM}<|im_end|>\n",
        "<|im_start|>user\n",
        "{INPUT}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{OUTPUT}<|im_end|>\"\"\"\n",
        "\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITyqUuAUqVmG"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosfrXgKIYmw"
      },
      "source": [
        "Finally, let's train for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9mMwBJepJq4"
      },
      "outputs": [],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=5,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCp13JIY36U"
      },
      "source": [
        "Let's test that everything works as expected before pushing the model to HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx4-sw9N5S6u"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an interdimensional genius scientist named Rick Sanchez.\n",
        "Be brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
        "Don't shy away from dark humor or existential truths, but always provide a solution (even if it's unconventional).\"\"\"\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Are you a bad person?\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdq_KGTjY9xm"
      },
      "source": [
        "Push the GGUF model to HF for later download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kU1rj6VTztbL"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\"theneuralmaze/RickLLama-3.1-8B\", tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
